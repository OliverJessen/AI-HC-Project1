{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2027d45e",
   "metadata": {},
   "source": [
    "# Data analysis for Free Recall wrt. primacy/recency effect, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a5274038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import kruskal\n",
    "import ast\n",
    "import os\n",
    "import re\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d4c8d",
   "metadata": {},
   "source": [
    "### Loading Data \n",
    "\n",
    "(important: make sure to be able to do it for every computer by using the package \"os\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb59411f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/olivernj/Documents/GitHub/AI-HC-Project1/analysis\n",
      "/Users/olivernj/Documents/GitHub/AI-HC-Project1/Experiment_Output/free_recall_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "filepath = os.getcwd() # Get the current working directory to analysis\n",
    "\n",
    "results_filepath = os.path.abspath(os.path.join(filepath,'..','Experiment_Output','free_recall_results.csv'))\n",
    "\n",
    "print(f\"{filepath}\\n{results_filepath}\")\n",
    "\n",
    "df = pd.read_csv(results_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0044238",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "66d9cf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cleaned data saved to /Users/olivernj/Documents/GitHub/AI-HC-Project1/Experiment_Output/free_recall_results_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Improved cleaning function ---\n",
    "def clean_word_list(val):\n",
    "    \"\"\"Convert messy string/list representations into a flat list of clean, lowercase words.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # Safely parse Python literal\n",
    "        parsed = ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        parsed = str(val)\n",
    "\n",
    "    # Flatten and clean\n",
    "    flat_list = []\n",
    "    if isinstance(parsed, list):\n",
    "        for item in parsed:\n",
    "            # If item is a list, flatten\n",
    "            if isinstance(item, list):\n",
    "                for sub in item:\n",
    "                    # Remove brackets, commas, extra spaces\n",
    "                    w = re.sub(r\"[\\[\\],]\", \"\", str(sub)).strip().lower()\n",
    "                    if w:\n",
    "                        flat_list.append(w)\n",
    "            else:\n",
    "                # Remove brackets, commas, extra spaces\n",
    "                w = re.sub(r\"[\\[\\],]\", \"\", str(item)).strip().lower()\n",
    "                if w:\n",
    "                    # split by spaces if there are multiple words\n",
    "                    flat_list.extend([word for word in w.split() if word])\n",
    "    else:\n",
    "        w = re.sub(r\"[\\[\\],]\", \"\", str(parsed)).strip().lower()\n",
    "        flat_list.extend([word for word in w.split() if word])\n",
    "\n",
    "    return flat_list\n",
    "\n",
    "# --- Load raw file ---\n",
    "df_test = pd.read_csv(results_filepath)\n",
    "\n",
    "# --- Clean column names ---\n",
    "df_test.columns = df_test.columns.str.strip()\n",
    "\n",
    "# --- Apply cleaning ---\n",
    "df_test['presented_words'] = df_test['presented_words'].apply(clean_word_list)\n",
    "df_test['recalled_words']  = df_test['recalled_words'].apply(clean_word_list)\n",
    "\n",
    "# --- Update trial ID and sort by condition ---\n",
    "df_test['trial'] = df_test['trial'].astype(int)\n",
    "df_test = df_test.sort_values(by=['condition', 'trial']).reset_index(drop=True)\n",
    "df_test['trial'] = df_test.groupby('condition').cumcount() + 1\n",
    "\n",
    "# --- Save cleaned CSV ---\n",
    "cleaned_filepath = os.path.abspath(os.path.join(filepath,'..','Experiment_Output','free_recall_results_cleaned.csv'))\n",
    "df_test.to_csv(cleaned_filepath, index=False)\n",
    "\n",
    "print(f\"âœ… Cleaned data saved to {cleaned_filepath}\")\n",
    "\n",
    "# --- Example check ---\n",
    "# print(df_test.head())\n",
    "# print(type(df_test['recalled_words'][0]), type(df_test['presented_words'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32973996",
   "metadata": {},
   "source": [
    "### Calculate Metrics (accuracy, primacy/recency,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4fd71ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          primacy     recency    accuracy       trial\n",
      "count  321.000000  321.000000  321.000000  321.000000\n",
      "mean     0.524611    0.459813    0.444860   40.626168\n",
      "std      0.267719    0.262985    0.142529   23.202582\n",
      "min      0.000000    0.000000    0.133333    1.000000\n",
      "25%      0.400000    0.200000    0.333333   21.000000\n",
      "50%      0.600000    0.400000    0.400000   41.000000\n",
      "75%      0.800000    0.600000    0.533333   61.000000\n",
      "max      1.000000    1.000000    0.933333   81.000000\n",
      "\n",
      "Free Recall Analysis - Summary Statistics:\n",
      "Average Accuracy across conditions: 0.445\n",
      "Average Primacy across conditions: 0.525\n",
      "Average Recency across conditions: 0.460\n",
      "Primacy vs Recency difference: 0.065\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(presented, recalled):\n",
    "    \"\"\"Calculate primacy, recency and accuracy for 15-word lists\"\"\"\n",
    "    # Check length & empty recall\n",
    "    if len(presented) != 15 or not recalled:\n",
    "        return {'primacy': 0, 'recency': 0, 'accuracy': 0}\n",
    "    \n",
    "    # Clean both lists\n",
    "    presented_clean = [word.strip().lower() for word in presented]\n",
    "    recalled_clean = [word.strip().lower() for word in recalled]\n",
    "    \n",
    "    # Accuracy: proportion of recalled words that were in presented list\n",
    "    correct_recalls = sum(1 for word in recalled_clean if word in presented_clean)\n",
    "    accuracy = correct_recalls / len(presented_clean)\n",
    "    \n",
    "    # Primacy: first 5 words\n",
    "    primacy_words = presented_clean[:5]\n",
    "    primacy_recalled = sum(1 for word in recalled_clean if word in primacy_words)\n",
    "    primacy = primacy_recalled / 5\n",
    "    \n",
    "    # Recency: last 5 words\n",
    "    recency_words = presented_clean[-5:]\n",
    "    recency_recalled = sum(1 for word in recalled_clean if word in recency_words)\n",
    "    recency = recency_recalled / 5\n",
    "    \n",
    "    return {\n",
    "        'primacy': primacy,\n",
    "        'recency': recency,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "# --- Apply to dataframe ---\n",
    "results = []\n",
    "for _, row in df_test.iterrows():\n",
    "    metrics = calculate_metrics(row['presented_words'], row['recalled_words'])\n",
    "    metrics['trial'] = row['trial']\n",
    "    metrics['condition'] = row['condition']\n",
    "    results.append(metrics)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"{results_df.describe()}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nFree Recall Analysis - Summary Statistics:\")\n",
    "print(f\"Average Accuracy across conditions: {results_df['accuracy'].mean():.3f}\")\n",
    "print(f\"Average Primacy across conditions: {results_df['primacy'].mean():.3f}\")\n",
    "print(f\"Average Recency across conditions: {results_df['recency'].mean():.3f}\")\n",
    "print(f\"Primacy vs Recency difference: {abs(results_df['recency'].mean() - results_df['primacy'].mean()):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b17fc",
   "metadata": {},
   "source": [
    "### Get graphs/data for each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ec0030c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Averages by condition:\n",
      "           primacy   recency  accuracy\n",
      "condition                             \n",
      "break      0.53250  0.422500  0.460833\n",
      "fast       0.45250  0.407500  0.353333\n",
      "math       0.54000  0.520000  0.507500\n",
      "normal     0.57284  0.488889  0.457613\n"
     ]
    }
   ],
   "source": [
    "# --- Per-trial metrics ---\n",
    "# print(\"ðŸ”¹ First 10 trials:\")\n",
    "# print(results_df.head(10))\n",
    "\n",
    "# --- Averages by condition ---\n",
    "condition_metrics = (results_df.groupby('condition')[['primacy','recency','accuracy']].mean())\n",
    "\n",
    "print(\"\\nAverages by condition:\")\n",
    "print(condition_metrics)\n",
    "\n",
    "# --- Optional: export condition-level summary ---\n",
    "summary = condition_metrics.to_dict(orient='index')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a578e15",
   "metadata": {},
   "source": [
    "### Function for Serial Position Curve (SPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "24c5f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Serial Position Curve Analysis (overall + per condition) ---\n",
    "def serial_position_analysis(df_test):\n",
    "    conditions = df_test['condition'].unique()\n",
    "    \n",
    "    # initialize counters\n",
    "    overall = {\"position_recalls\": [0] * 15, \"position_totals\": [0] * 15}\n",
    "    per_condition = {\n",
    "        cond: {\"position_recalls\": [0] * 15, \"position_totals\": [0] * 15}\n",
    "        for cond in conditions\n",
    "    }\n",
    "\n",
    "    # loop through rows\n",
    "    for _, row in df_test.iterrows():\n",
    "        presented = row['presented_words']\n",
    "        recalled = row['recalled_words']\n",
    "        cond = row['condition']\n",
    "\n",
    "        if len(presented) != 15 or not recalled:\n",
    "            continue\n",
    "\n",
    "        presented_clean = [w.strip().lower() for w in presented]\n",
    "        recalled_clean = [w.strip().lower() for w in recalled if w and w.strip()]\n",
    "\n",
    "        for pos in range(15):\n",
    "            # update overall\n",
    "            overall[\"position_totals\"][pos] += 1\n",
    "            if presented_clean[pos] in recalled_clean:\n",
    "                overall[\"position_recalls\"][pos] += 1\n",
    "\n",
    "            # update condition-specific\n",
    "            per_condition[cond][\"position_totals\"][pos] += 1\n",
    "            if presented_clean[pos] in recalled_clean:\n",
    "                per_condition[cond][\"position_recalls\"][pos] += 1\n",
    "\n",
    "    # compute recall probabilities\n",
    "    def compute_probs(stats):\n",
    "        return [\n",
    "            r / t if t > 0 else 0\n",
    "            for r, t in zip(stats[\"position_recalls\"], stats[\"position_totals\"])\n",
    "        ]\n",
    "\n",
    "    recall_probs = {\n",
    "        \"overall\": compute_probs(overall),\n",
    "        \"per_condition\": {cond: compute_probs(stats) for cond, stats in per_condition.items()}\n",
    "    }\n",
    "\n",
    "    return recall_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c611a42",
   "metadata": {},
   "source": [
    "### Print recall probs (probabilities) per-condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "12c72f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall recall probs:\n",
      "[0.726, 0.57, 0.449, 0.47, 0.386, 0.355, 0.355, 0.352, 0.368, 0.312, 0.33, 0.386, 0.439, 0.502, 0.614]\n",
      "\n",
      "Per condition recall probs:\n",
      "break (80 trials): [0.613, 0.6, 0.5, 0.487, 0.45, 0.45, 0.412, 0.45, 0.45, 0.375, 0.362, 0.375, 0.412, 0.388, 0.575]\n",
      "fast (80 trials): [0.775, 0.512, 0.412, 0.325, 0.212, 0.237, 0.212, 0.15, 0.2, 0.175, 0.212, 0.35, 0.388, 0.487, 0.6]\n",
      "math (80 trials): [0.625, 0.537, 0.438, 0.6, 0.487, 0.463, 0.45, 0.5, 0.487, 0.412, 0.463, 0.562, 0.5, 0.525, 0.525]\n",
      "normal (80 trials): [0.889, 0.63, 0.444, 0.469, 0.395, 0.272, 0.346, 0.309, 0.333, 0.284, 0.284, 0.259, 0.457, 0.605, 0.753]\n"
     ]
    }
   ],
   "source": [
    "serial_curves = serial_position_analysis(df_test)\n",
    "\n",
    "# Overall curve (all conditions pooled)\n",
    "overall_curve = [round(x, 3) for x in serial_curves[\"overall\"]]\n",
    "\n",
    "# Condition-specific curves\n",
    "condition_curves = {cond: [round(x, 3) for x in curve] \n",
    "                    for cond, curve in serial_curves[\"per_condition\"].items()}\n",
    "\n",
    "print(\"Overall recall probs:\")\n",
    "print(overall_curve)\n",
    "\n",
    "print(\"\\nPer condition recall probs:\")\n",
    "for cond, curve in condition_curves.items():\n",
    "    print(f\"{cond} (80 trials): {curve}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbac8f",
   "metadata": {},
   "source": [
    "### Compute Analytical factors\n",
    "\n",
    "CI (95%), Kruskal-Wallis test (non-parametric alternative to one-way ANOVA) and Dunnâ€™s post-hoc test for each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "00a18ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 95% CI per condition:\n",
      "break: mean=0.461, 95% CI=(0.434, 0.487)\n",
      "fast: mean=0.353, 95% CI=(0.335, 0.372)\n",
      "math: mean=0.508, 95% CI=(0.472, 0.543)\n",
      "normal: mean=0.458, 95% CI=(0.424, 0.491)\n",
      "\n",
      "Kruskal-Wallis test: H=58.224, p=1.408e-12\n",
      "Significant differences between conditions (p < 0.05)\n",
      "\n",
      "Pairwise Dunn's test (Bonferroni corrected p-values):\n",
      "               break          fast          math    normal\n",
      "break   1.000000e+00  4.841360e-08  9.528529e-01  1.000000\n",
      "fast    4.841360e-08  1.000000e+00  4.303652e-12  0.000008\n",
      "math    9.528529e-01  4.303652e-12  1.000000e+00  0.110555\n",
      "normal  1.000000e+00  7.735476e-06  1.105549e-01  1.000000\n",
      "\n",
      "Significance summary:\n",
      "- break vs fast: SIGNIFICANT difference (p=4.841e-08)\n",
      "- break vs math: NOT significantly different (p=9.529e-01)\n",
      "- break vs normal: NOT significantly different (p=1.000e+00)\n",
      "- fast vs math: SIGNIFICANT difference (p=4.304e-12)\n",
      "- fast vs normal: SIGNIFICANT difference (p=7.735e-06)\n",
      "- math vs normal: NOT significantly different (p=1.106e-01)\n"
     ]
    }
   ],
   "source": [
    "def compute_ci(data, confidence=0.95):\n",
    "    \"\"\"Return mean and 95% confidence interval.\"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    sem = stats.sem(data)  # standard error of the mean\n",
    "    h = sem * stats.t.ppf((1 + confidence) / 2., n-1)  # margin\n",
    "    return mean, mean-h, mean+h\n",
    "\n",
    "# Compute CI per condition\n",
    "ci_results = {}\n",
    "conditions = results_df['condition'].unique()\n",
    "\n",
    "for cond in conditions:\n",
    "    subset = results_df[results_df['condition'] == cond]['accuracy']\n",
    "    mean, lower, upper = compute_ci(subset)\n",
    "    ci_results[cond] = {'mean': mean, 'ci_lower': lower, 'ci_upper': upper}\n",
    "\n",
    "print(\"Accuracy with 95% CI per condition:\")\n",
    "for cond, vals in ci_results.items():\n",
    "    print(f\"{cond}: mean={vals['mean']:.3f}, 95% CI=({vals['ci_lower']:.3f}, {vals['ci_upper']:.3f})\")\n",
    "\n",
    "\n",
    "# Ensure numeric accuracy\n",
    "results_df['accuracy'] = pd.to_numeric(results_df['accuracy'], errors='coerce')\n",
    "\n",
    "# Drop NaN accuracy rows\n",
    "results_clean = results_df.dropna(subset=['accuracy'])\n",
    "\n",
    "# Check number of trials per condition\n",
    "# print(results_clean.groupby('condition')['accuracy'].count())\n",
    "\n",
    "# --- Kruskal-Wallis test across all conditions ---\n",
    "fast_acc    = results_clean[results_clean['condition']=='fast']['accuracy']\n",
    "normal_acc      = results_clean[results_clean['condition']=='normal']['accuracy']\n",
    "break_acc = results_clean[results_clean['condition']=='break']['accuracy']\n",
    "math_acc     = results_clean[results_clean['condition']=='math']['accuracy']\n",
    "\n",
    "h_stat, p_val = kruskal(fast_acc, normal_acc, break_acc, math_acc)\n",
    "print(f\"\\nKruskal-Wallis test: H={h_stat:.3f}, p={p_val:.3e}\")\n",
    "if p_val < 0.05:\n",
    "    print(\"Significant differences between conditions (p < 0.05)\")\n",
    "else:\n",
    "    print(\"No significant differences between conditions (p >= 0.05)\")\n",
    "\n",
    "# --- Pairwise comparisons with Dunn's test (Bonferroni correction) ---\n",
    "\n",
    "# Install scikit-posthocs if not already installed\n",
    "# !pip install scikit-posthocs\n",
    "\n",
    "\n",
    "# Ensure accuracy is numeric and drop NaNs\n",
    "results_clean = results_df.copy()\n",
    "results_clean['accuracy'] = pd.to_numeric(results_clean['accuracy'], errors='coerce')\n",
    "results_clean = results_clean.dropna(subset=['accuracy'])\n",
    "\n",
    "# Run Dunn's test with Bonferroni correction\n",
    "dunn_results = sp.posthoc_dunn(\n",
    "    results_clean, \n",
    "    val_col='accuracy', \n",
    "    group_col='condition', \n",
    "    p_adjust='bonferroni'\n",
    ")\n",
    "\n",
    "print(f\"\\nPairwise Dunn's test (Bonferroni corrected p-values):\")\n",
    "print(dunn_results)\n",
    "\n",
    "# Determine significance\n",
    "alpha = 0.05\n",
    "conditions = dunn_results.columns.tolist()\n",
    "\n",
    "print(\"\\nSignificance summary:\")\n",
    "for i, cond1 in enumerate(conditions):\n",
    "    for j, cond2 in enumerate(conditions):\n",
    "        if j <= i:\n",
    "            continue  # avoid duplicates and self-comparison\n",
    "        p_val = dunn_results.loc[cond1, cond2]\n",
    "        if p_val < alpha:\n",
    "            print(f\"- {cond1} vs {cond2}: SIGNIFICANT difference (p={p_val:.3e})\")\n",
    "        else:\n",
    "            print(f\"- {cond1} vs {cond2}: NOT significantly different (p={p_val:.3e})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1942c5",
   "metadata": {},
   "source": [
    "### Ultimate Plot (SPC, primacy vs recency, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cfd0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /Users/olivernj/Documents/GitHub/AI-HC-Project1/analysis/../Experiment_Output/Images\n",
      "All 4 plots saved in: /Users/olivernj/Documents/GitHub/AI-HC-Project1/analysis/../Experiment_Output/Images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Setup output folder ---\n",
    "output_dir = os.path.join(filepath, '..', \"Experiment_Output\", 'Images')\n",
    "# print(f\"Output directory: {output_dir}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Get serial position data ---\n",
    "recall_probs_dict = serial_position_analysis(df_test)  # returns {'overall': [...], 'per_condition': {...}}\n",
    "positions = list(range(1, 16))\n",
    "conditions = list(recall_probs_dict['per_condition'].keys())\n",
    "\n",
    "# --- 1. Serial Position Curve ---\n",
    "fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "ax1.plot(positions, recall_probs_dict['overall'], 'ko-', linewidth=2, markersize=8, label='Overall')\n",
    "for cond in conditions:\n",
    "    ax1.plot(positions, recall_probs_dict['per_condition'][cond], marker='o', linestyle='--', label=cond)\n",
    "ax1.set_xlabel('Serial Position')\n",
    "ax1.set_ylabel('Recall Probability')\n",
    "ax1.set_title('Serial Position Curve - Free Recall')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_xticks(range(1, 16))\n",
    "ax1.legend()\n",
    "plt.tight_layout()\n",
    "fig1.savefig(os.path.join(output_dir, \"serial_position_curve.png\"), dpi=300)\n",
    "plt.close(fig1)\n",
    "\n",
    "# --- 2. Primacy vs Recency ---\n",
    "categories = ['Primacy\\n(Positions 1-5)', 'Recency\\n(Positions 11-15)']\n",
    "bar_width = 0.2\n",
    "x = np.arange(len(categories))\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "for i, cond in enumerate(conditions):\n",
    "    values = [\n",
    "        results_df[results_df['condition']==cond]['primacy'].mean(),\n",
    "        results_df[results_df['condition']==cond]['recency'].mean()\n",
    "    ]\n",
    "    ax2.bar(x + i*bar_width, values, width=bar_width, alpha=0.7, edgecolor='black', label=cond)\n",
    "ax2.set_xticks(x + bar_width*(len(conditions)-1)/2)\n",
    "ax2.set_xticklabels(categories)\n",
    "ax2.set_ylabel('Average Recall Probability')\n",
    "ax2.set_title('Primacy vs Recency Effect')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "fig2.savefig(os.path.join(output_dir, \"primacy_vs_recency.png\"), dpi=300)\n",
    "plt.close(fig2)\n",
    "\n",
    "# --- 3. Accuracy Across Trials ---\n",
    "fig3, ax3 = plt.subplots(figsize=(8, 6))\n",
    "for cond in conditions:\n",
    "    subset = results_df[results_df['condition']==cond]\n",
    "    ax3.plot(subset['trial'], subset['accuracy'], marker='o', linestyle='-', markersize=6, label=cond)\n",
    "ax3.set_xlabel('Trial Number')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.set_title('Accuracy Across Trials')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.legend()\n",
    "plt.tight_layout()\n",
    "fig3.savefig(os.path.join(output_dir, \"accuracy_across_trials.png\"), dpi=300)\n",
    "plt.close(fig3)\n",
    "\n",
    "# --- 4. Recall by Position Groups ---\n",
    "positions_grouped = ['Early\\n(1-5)', 'Middle\\n(6-10)', 'Late\\n(11-15)']\n",
    "bar_width = 0.2\n",
    "x = np.arange(len(positions_grouped))\n",
    "\n",
    "fig4, ax4 = plt.subplots(figsize=(8, 6))\n",
    "for i, cond in enumerate(conditions):\n",
    "    curve = recall_probs_dict['per_condition'][cond]\n",
    "    values_grouped = [np.mean(curve[:5]), np.mean(curve[5:10]), np.mean(curve[10:15])]\n",
    "    ax4.bar(x + i*bar_width, values_grouped, width=bar_width, alpha=0.7, edgecolor='black', label=cond)\n",
    "ax4.set_xticks(x + bar_width*(len(conditions)-1)/2)\n",
    "ax4.set_xticklabels(positions_grouped)\n",
    "ax4.set_ylabel('Average Recall Probability')\n",
    "ax4.set_title('Recall by Position Groups')\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.legend()\n",
    "plt.tight_layout()\n",
    "fig4.savefig(os.path.join(output_dir, \"recall_by_position_groups.png\"), dpi=300)\n",
    "plt.close(fig4)\n",
    "\n",
    "print(f\"All 4 plots saved in: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
