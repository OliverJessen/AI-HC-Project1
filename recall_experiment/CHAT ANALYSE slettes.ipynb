{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b1c4c3a",
   "metadata": {},
   "source": [
    "# Free Recall Memory Experiment Analysis\n",
    "\n",
    "This notebook analyzes data from a free recall memory experiment to understand:\n",
    "- Overall memory performance\n",
    "- Serial position effects (primacy and recency)\n",
    "- Individual trial performance\n",
    "\n",
    "## Background Theory\n",
    "- **Primacy Effect**: Better recall for words at the beginning of the list (long-term memory encoding)\n",
    "- **Recency Effect**: Better recall for words at the end of the list (working memory)\n",
    "- **Serial Position Curve**: The U-shaped curve showing recall probability by position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d61e17",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1a9aa",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ebfdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_file = 'data/free_recall_results.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Number of trials: {len(df)}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_words(word_string):\n",
    "    \"\"\"Convert '[word1, word2, word3]' into a clean list of words\"\"\"\n",
    "    if pd.isna(word_string) or word_string == '[]':\n",
    "        return []\n",
    "    \n",
    "    # Remove brackets and quotes, then split by comma\n",
    "    cleaned = word_string.strip('[]').replace('\"', '').replace(\"'\", '')\n",
    "    words = [word.strip() for word in cleaned.split(',') if word.strip()]\n",
    "    return words\n",
    "\n",
    "# Test the function\n",
    "sample_string = df.iloc[0]['presented_words']\n",
    "print(f\"Original: {sample_string}\")\n",
    "print(f\"Parsed: {parse_words(sample_string)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351d234",
   "metadata": {},
   "source": [
    "## Calculate Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy for each trial\n",
    "all_accuracies = []\n",
    "trial_details = []\n",
    "\n",
    "print(\"Trial-by-Trial Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    trial_num = row['trial']\n",
    "    \n",
    "    # Parse the words\n",
    "    presented_words = parse_words(row['presented_words'])\n",
    "    recalled_words = parse_words(row['recalled_words'])\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    presented_set = set(presented_words)\n",
    "    recalled_set = set(recalled_words)\n",
    "    correct_words = presented_set.intersection(recalled_set)\n",
    "    \n",
    "    accuracy = len(correct_words) / len(presented_words) if len(presented_words) > 0 else 0\n",
    "    all_accuracies.append(accuracy)\n",
    "    \n",
    "    trial_details.append({\n",
    "        'trial': trial_num,\n",
    "        'presented': presented_words,\n",
    "        'recalled': recalled_words,\n",
    "        'correct': list(correct_words),\n",
    "        'accuracy': accuracy,\n",
    "        'n_presented': len(presented_words),\n",
    "        'n_recalled': len(recalled_words),\n",
    "        'n_correct': len(correct_words)\n",
    "    })\n",
    "    \n",
    "    print(f\"Trial {trial_num}: {len(correct_words)}/{len(presented_words)} = {accuracy:.1%}\")\n",
    "\n",
    "print(f\"\\nOverall Statistics:\")\n",
    "print(f\"Average accuracy: {np.mean(all_accuracies):.1%}\")\n",
    "print(f\"Best performance: {max(all_accuracies):.1%}\")\n",
    "print(f\"Worst performance: {min(all_accuracies):.1%}\")\n",
    "print(f\"Standard deviation: {np.std(all_accuracies):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4fcb9c",
   "metadata": {},
   "source": [
    "## Serial Position Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd35a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which positions are remembered best\n",
    "position_correct = {}\n",
    "\n",
    "for trial in trial_details:\n",
    "    presented = trial['presented']\n",
    "    recalled = trial['recalled']\n",
    "    \n",
    "    # Track which positions were remembered correctly\n",
    "    for i, word in enumerate(presented):\n",
    "        position = i + 1  # Start counting from 1\n",
    "        if position not in position_correct:\n",
    "            position_correct[position] = []\n",
    "        \n",
    "        # Was this word remembered?\n",
    "        was_remembered = word in recalled\n",
    "        position_correct[position].append(was_remembered)\n",
    "\n",
    "# Calculate recall probability for each position\n",
    "position_probabilities = {}\n",
    "print(\"Position Effects:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "for pos in sorted(position_correct.keys()):\n",
    "    prob = np.mean(position_correct[pos])\n",
    "    position_probabilities[pos] = prob\n",
    "    print(f\"Position {pos}: {prob:.1%}\")\n",
    "\n",
    "# Calculate primacy and recency effects\n",
    "positions = sorted(position_probabilities.keys())\n",
    "if len(positions) >= 5:\n",
    "    primacy_positions = positions[:3]\n",
    "    recency_positions = positions[-3:]\n",
    "    middle_positions = positions[3:-3] if len(positions) > 6 else []\n",
    "    \n",
    "    primacy_prob = np.mean([position_probabilities[pos] for pos in primacy_positions])\n",
    "    recency_prob = np.mean([position_probabilities[pos] for pos in recency_positions])\n",
    "    middle_prob = np.mean([position_probabilities[pos] for pos in middle_positions]) if middle_positions else 0\n",
    "    \n",
    "    print(f\"\\nMemory Effects:\")\n",
    "    print(f\"Primacy Effect (first 3): {primacy_prob:.1%}\")\n",
    "    if middle_positions:\n",
    "        print(f\"Middle positions: {middle_prob:.1%}\")\n",
    "    print(f\"Recency Effect (last 3): {recency_prob:.1%}\")\n",
    "    \n",
    "    if primacy_prob > middle_prob + 0.1:\n",
    "        print(\"Primacy effect detected!\")\n",
    "    if recency_prob > middle_prob + 0.1:\n",
    "        print(\"Recency effect detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83222a7",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Serial Position Curve\n",
    "positions_list = list(position_probabilities.keys())\n",
    "probabilities_list = list(position_probabilities.values())\n",
    "\n",
    "axes[0,0].plot(positions_list, probabilities_list, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0,0].set_xlabel('Word Position in List')\n",
    "axes[0,0].set_ylabel('Recall Probability')\n",
    "axes[0,0].set_title('Serial Position Curve')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].set_ylim(0, 1)\n",
    "\n",
    "# Add colored regions for primacy/recency\n",
    "if len(positions_list) >= 5:\n",
    "    axes[0,0].axvspan(1, 3, alpha=0.2, color='blue', label='Primacy')\n",
    "    axes[0,0].axvspan(max(positions_list)-2, max(positions_list), alpha=0.2, color='red', label='Recency')\n",
    "    axes[0,0].legend()\n",
    "\n",
    "# 2. Trial Performance\n",
    "trial_numbers = [t['trial'] for t in trial_details]\n",
    "trial_accuracies = [t['accuracy'] for t in trial_details]\n",
    "\n",
    "axes[0,1].plot(trial_numbers, trial_accuracies, 'go-', linewidth=2, markersize=6)\n",
    "axes[0,1].set_xlabel('Trial Number')\n",
    "axes[0,1].set_ylabel('Accuracy')\n",
    "axes[0,1].set_title('Performance Across Trials')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].set_ylim(0, 1)\n",
    "\n",
    "# 3. Accuracy Distribution\n",
    "axes[1,0].hist(all_accuracies, bins=8, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1,0].set_xlabel('Accuracy')\n",
    "axes[1,0].set_ylabel('Number of Trials')\n",
    "axes[1,0].set_title('Distribution of Accuracy Scores')\n",
    "axes[1,0].axvline(np.mean(all_accuracies), color='red', linestyle='--', \n",
    "                 label=f'Mean: {np.mean(all_accuracies):.1%}')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 4. Memory Effects Comparison\n",
    "if len(positions_list) >= 5:\n",
    "    categories = ['First\\n(Primacy)', 'Middle', 'Last\\n(Recency)']\n",
    "    values = [primacy_prob, middle_prob, recency_prob]\n",
    "    colors = ['lightblue', 'lightgray', 'lightcoral']\n",
    "    \n",
    "    bars = axes[1,1].bar(categories, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[1,1].set_ylabel('Recall Probability')\n",
    "    axes[1,1].set_title('Memory Effects Comparison')\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                      f'{value:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fcedd6",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe766b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary\n",
    "print(\"FREE RECALL EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total trials analyzed: {len(df)}\")\n",
    "print(f\"Average accuracy: {np.mean(all_accuracies):.1%}\")\n",
    "print(f\"Performance range: {min(all_accuracies):.1%} - {max(all_accuracies):.1%}\")\n",
    "\n",
    "if len(positions_list) >= 5:\n",
    "    print(f\"\\nSerial Position Effects:\")\n",
    "    print(f\"Primacy effect (first 3 positions): {primacy_prob:.1%}\")\n",
    "    print(f\"Recency effect (last 3 positions): {recency_prob:.1%}\")\n",
    "    if middle_positions:\n",
    "        print(f\"Middle positions: {middle_prob:.1%}\")\n",
    "    \n",
    "    print(f\"\\nConclusions:\")\n",
    "    if primacy_prob > middle_prob + 0.1:\n",
    "        print(\"- Primacy effect confirmed: Long-term memory encoding of early items\")\n",
    "    if recency_prob > middle_prob + 0.1:\n",
    "        print(\"- Recency effect confirmed: Working memory retention of recent items\")\n",
    "    \n",
    "    if primacy_prob > middle_prob + 0.1 and recency_prob > middle_prob + 0.1:\n",
    "        print(\"- Classic U-shaped serial position curve observed\")\n",
    "    \n",
    "print(f\"\\nThis demonstrates the dual-store model of memory:\")\n",
    "print(\"- Early items → Long-term memory (primacy)\")\n",
    "print(\"- Recent items → Working memory (recency)\")\n",
    "print(\"- Middle items → Neither effect, lower recall\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
